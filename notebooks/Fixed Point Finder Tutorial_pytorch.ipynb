{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Fixed Point Finder algorithm on a GRU that makes a binary decision about its input.\n",
    "\n",
    "The goal of this tutorial is to learn about fixed point finding by running the algorithm on a simple data generator, a Gated Recurrent Unit (GRU) that is trained to make a binary decision, namely whether the integral of the white noise input is in total positive or negative, outputing either a +1 or a -1.\n",
    "\n",
    "Running the fixed point finder on this decision-making GRU will yield:\n",
    "1. the underlying fixed points\n",
    "2. the first order taylor series approximations around those fixed points.\n",
    "\n",
    "Doing this will exercise the concepts defined in the [Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks](https://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00409). It's pretty important that you have read at least the beginning of the paper, otherwise you won't understand *why* we are doing what we are doing.\n",
    "\n",
    "Applying this technique was done with some success in the following papers\n",
    "* [Context-dependent computation by recurrent dynamics in prefrontal cortex](https://www.nature.com/articles/nature12742)\n",
    "* [A neural network that finds a naturalistic solution for the production of muscle activity](https://www.nature.com/articles/nn.4042)\n",
    "\n",
    "In this tutorial we do a few things:\n",
    "1. Train the decision making GRU\n",
    "2. Find the fixed points of the GRU.\n",
    "3. Find the jacobians ($\\partial{F}/\\partial{h}$), where $h$ is the hidden state and $F$ is the GRU and eigenvalues of those jacobians.\n",
    "4. Show how the linear dynamics explain this example. \n",
    "\n",
    "### Why do we care about fixed points? \n",
    "1. Because they show where the memory of the system is.\n",
    "2. (More importantly) One can linearize the dynamics around the fixed points and have reasonable approximations.\n",
    "\n",
    "$ F(h) = F(h^*) + F'(h^*)(h-h^*) + \\frac{1}{2}F''(h^*)(h-h^*)^2 + \\cdots$\n",
    "\n",
    "So if we expand around a fixed point $h^*$, then we can see that the first order approximation is likely to be accurate up to $(h-h^*)^2$.  Thus if $h-h^*$ is small, we likely have a very good approximation of the system.  \n",
    "\n",
    "Emprically, the volumes around which lineariation is valid needs to be studied for computational dynamics systems, such as recurrent neural networks like a GRU.\n",
    "\n",
    "Finally, assuming the linear system is a valid approximation, one can study the linearized system via standard analyses, such as eigenvalue / eigenvector decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy, JAX, Matplotlib and h5py should all be correctly installed and on the python path.\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import datetime\n",
    "import h5py\n",
    "import jax.numpy as np\n",
    "from jax import jacrev, random, vmap\n",
    "from jax.experimental import optimizers\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp             # original CPU-backed NumPy\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from pdb import set_trace as b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tutorial code.\n",
    "from imp import reload\n",
    "\n",
    "# You must change this to the location of computation-thru-dynamics directory.\n",
    "HOME_DIR = '/home/ubuntu/' \n",
    "\n",
    "sys.path.append(os.path.join(HOME_DIR,'computation-thru-dynamics'))\n",
    "import fixed_point_finder.decision as decision\n",
    "import fixed_point_finder.fixed_points as fp_optimize\n",
    "import fixed_point_finder.rnn as rnn\n",
    "import fixed_point_finder.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed point analysis\n",
    "\n",
    "Now that we've trained up this GRU to decide whether or not the perfect integral of the input is positive or negative, we can analyze the system via fixed point analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some preliminaries. \n",
    "x_star = np.zeros(u)  # We always linearize the input around zero in this example.\n",
    "\n",
    "# Make a one parameter function of thie hidden state, useful for jacobians.\n",
    "rnn_fun_jax = lambda h : rnn.gru(params, h, x_star)\n",
    "batch_rnn_fun_jax = vmap(rnn_fun_jax, in_axes=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did the JAX version work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in params.keys():\n",
    "    print(param_name + ': ', params[param_name].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_params = torch.load('/home/ubuntu/sentiment_models/sentiment-gru128-s1/checkpoints/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_rnn_params = {\n",
    "    'weight_ih_l0': pytorch_params['rnn.weight_ih_l0'],\n",
    "    'weight_hh_l0': pytorch_params['rnn.weight_hh_l0'],\n",
    "    'bias_ih_l0': pytorch_params['rnn.bias_ih_l0'],\n",
    "    'bias_hh_l0': pytorch_params['rnn.bias_hh_l0'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_rnn_params_numpy = {\n",
    "    'weight_ih_l0': pytorch_params['rnn.weight_ih_l0'].cpu().numpy(),\n",
    "    'weight_hh_l0': pytorch_params['rnn.weight_hh_l0'].cpu().numpy(),\n",
    "    'bias_ih_l0': pytorch_params['rnn.bias_ih_l0'].cpu().numpy(),\n",
    "    'bias_hh_l0': pytorch_params['rnn.bias_hh_l0'].cpu().numpy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([384, 256]),\n",
       " torch.Size([384, 128]),\n",
       " torch.Size([384]),\n",
       " torch.Size([384]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convert between PyTorch and JAX format\n",
    "\n",
    "(pytorch_rnn_params['weight_ih_l0'].shape,  # (W_ir|W_iz|W_in), (3*hidden_size, input_size)\n",
    "pytorch_rnn_params['weight_hh_l0'].shape,  # (W_hr|W_hz|W_hn), (3*hidden_size, hidden_size)\n",
    "pytorch_rnn_params['bias_ih_l0'].shape, # (b_ir|b_iz|b_in), (3*hidden_size)\n",
    "pytorch_rnn_params['bias_hh_l0'].shape,) # (b_hr|b_hz|b_hn), (3*hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ir, W_iz, W_in = np.split(pytorch_rnn_params['weight_ih_l0'].cpu().numpy(), 3)\n",
    "W_ir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hr, W_hz, W_hn = np.split(pytorch_rnn_params['weight_hh_l0'].cpu().numpy(), 3)\n",
    "W_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ir, b_iz, b_in = np.split(pytorch_rnn_params['bias_ih_l0'].cpu().numpy(), 3)\n",
    "b_ir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_hr, b_hz, b_hn = np.split(pytorch_rnn_params['bias_hh_l0'].cpu().numpy(), 3)\n",
    "b_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128,), (128,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_in.shape, b_hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-abefa8d9bb1a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-abefa8d9bb1a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    .shape\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-88f8d52208d8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-88f8d52208d8>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    'bO':  (1,)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    'bC':  np.concatenate([b_in, b_hn]) # shape:  (256,) # was: (100,)\n",
    "    \n",
    "    'bO':  (1,)\n",
    "    \n",
    "    \n",
    "    'bRU':  (200,)\n",
    "    \n",
    "    \n",
    "    'h0':  (100,)\n",
    "    \n",
    "    \n",
    "    'wCHX':  (100, 101)\n",
    "    'wO':  (1, 100)\n",
    "    \n",
    "    'wRUHX':     # was:  (200, 101)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_rnn = torch.nn.GRU(input_size=256, hidden_size=128)\n",
    "pytorch_rnn.load_state_dict(pytorch_rnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-411c8960cbd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight:  torch.Size([31312, 256])\n",
      "rnn.weight_ih_l0:  torch.Size([384, 256])\n",
      "rnn.weight_hh_l0:  torch.Size([384, 128])\n",
      "rnn.bias_ih_l0:  torch.Size([384])\n",
      "rnn.bias_hh_l0:  torch.Size([384])\n",
      "fc.weight:  torch.Size([1, 128])\n",
      "fc.bias:  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param_name in pytorch_params.keys():\n",
    "    print(param_name + ': ', pytorch_params[param_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"key_map = {\\n    # pretty sure:\\n    'bO': 'fc.bias',\\n    'wO': 'fc.weight',\\n    \\n    'wRUHX': 'emb.weight',  # used\\n    \\n    'wCHX': 'rnn.weight_ih_l0', # used\\n    \\n    # guessing: \\n     \\n     'h0': 'rnn.bias_hh_l0',\\n     'bC': 'rnn.weight_hh_l0',  # used\\n     'bRU': 'rnn.bias_ih_l0'  # used\\n}\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''key_map = {\n",
    "    # pretty sure:\n",
    "    'bO': 'fc.bias',\n",
    "    'wO': 'fc.weight',\n",
    "    \n",
    "    'wRUHX': 'emb.weight',  # used\n",
    "    \n",
    "    'wCHX': 'rnn.weight_ih_l0', # used\n",
    "    \n",
    "    # guessing: \n",
    "     \n",
    "     'h0': 'rnn.bias_hh_l0',\n",
    "     'bC': 'rnn.weight_hh_l0',  # used\n",
    "     'bRU': 'rnn.bias_ih_l0'  # used\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_params_renamed = {k: np.array(pytorch_params[v].cpu()) for k, v in key_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param_name in pytorch_params_renamed.keys():\n",
    "#    print(param_name + ': ', pytorch_params_renamed[param_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "\n",
    "# have to do some surgery on these parameters...\n",
    "# rnn_fun = lambda h : rnn.gru(pytorch_params_renamed, h, x_star)  # change this definition?\n",
    "# rnn_fun = lambda h : pytorch_rnn(torch.tensor(x_star), torch.tensor(h))\n",
    "rnn_fun = lambda h : rnn.gru_pytorch(pytorch_rnn_params_numpy, h, x_star)  # change this definition?\n",
    "batch_rnn_fun = vmap(rnn_fun, in_axes=(0,))\n",
    "\n",
    "\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some functions that define the fixed point loss\n",
    "which is just the squared error of a point $(h - F(h))^2$ for a discrete time system such as a GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_loss_fun = fp_optimize.get_fp_loss_fun(rnn_fun)\n",
    "total_fp_loss_fun = fp_optimize.get_total_fp_loss_fun(rnn_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_internals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b5ead16bb785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_internals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_internals' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_internals['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to start the fixed point finder with some points, and it's always \n",
    "best to start with examples of where the state normally operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_internals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c7187cc5965d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfp_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_internals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hiddens'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# was batch x time x dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfp_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now batch * time x dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_internals' is not defined"
     ]
    }
   ],
   "source": [
    "fp_candidates = rnn_internals['hiddens']  # was batch x time x dim\n",
    "fp_candidates = np.reshape(fp_candidates, (-1, n)) # now batch * time x dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed point optimization hyperparameters\n",
    "fp_num_batches = 10000         # Total number of batches to train on.\n",
    "fp_batch_size = 128          # How many examples in each batch\n",
    "fp_step_size = 0.2          # initial learning rate\n",
    "fp_decay_factor = 0.9999     # decay the learning rate this much\n",
    "fp_decay_steps = 1           #\n",
    "fp_adam_b1 = 0.9             # Adam parameters\n",
    "fp_adam_b2 = 0.999\n",
    "fp_adam_eps = 1e-5\n",
    "fp_opt_print_every = 200   # Print training information during optimziation every so often\n",
    "\n",
    "# Fixed point finding thresholds and other HPs\n",
    "fp_noise_var = 0.0      # Gaussian noise added to fixed point candidates before optimization.\n",
    "fp_opt_stop_tol = 0.00001  # Stop optimizing when the average value of the batch is below this value.\n",
    "fp_tol = 0.00001        # Discard fps with squared speed larger than this value.\n",
    "fp_unique_tol = 0.025   # tolerance for determination of identical fixed points\n",
    "fp_outlier_tol = 1.0    # Anypoint whos closest fixed point is greater than tol is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimizing for fixed points, we set a few different stopping thresholds and run the fixed point finder a few times.  This is because there are rarely true numerical fixed points, though they do happen.  Instead one has slow points of varying slowness.  My experience is that if the dynamics of the slow point is very slow relative to the normal speed of the dynamics during the task / computation / behavior, then the slow point is effectively acting as a fixed point.  Moving forward, in the code, and in the comments, I always refer to slow points as fixed points, with the understanding that we are being informal. By having a few tolerances of varying slowness, we ensure we capture a large variety of the fixed points and likely get a better understanding of what the system is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(h)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-29-bf9e92e2026b>\u001b[0m(22)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m    \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_opt_details\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m        \u001b[0mfp_optimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_fixed_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_hps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change how rnn_fun is defined?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_idxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m        \u001b[0mF_of_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rnn_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fp_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bf9e92e2026b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_opt_details\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfp_optimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_fixed_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_hps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change how rnn_fun is defined?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_idxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mF_of_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rnn_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fp_candidates' is not defined"
     ]
    }
   ],
   "source": [
    "#try:\n",
    "reload(fp_optimize)\n",
    "\n",
    "fp_tols = [0.0001, 0.00001, 0.000001] # Used for both fp_tol and opt_stop_tol\n",
    "\n",
    "all_fps = {}\n",
    "for tol in fp_tols:\n",
    "    fp_hps = {'num_batches' : fp_num_batches, \n",
    "              'step_size' : fp_step_size, \n",
    "              'decay_factor' : fp_decay_factor, \n",
    "              'decay_steps' : fp_decay_steps, \n",
    "              'adam_b1' : fp_adam_b1, 'adam_b2' : fp_adam_b2, 'adam_eps' : fp_adam_eps,\n",
    "              'noise_var' : fp_noise_var, \n",
    "              'fp_opt_stop_tol' : tol, \n",
    "              'fp_tol' : tol, \n",
    "              'unique_tol' : fp_unique_tol, \n",
    "              'outlier_tol' : fp_outlier_tol, \n",
    "              'opt_print_every' : fp_opt_print_every}\n",
    "\n",
    "    b()\n",
    "    fps, fp_losses, fp_idxs, fp_opt_details = \\\n",
    "        fp_optimize.find_fixed_points(rnn_fun, fp_candidates, fp_hps, do_print=True)  # change how rnn_fun is defined?\n",
    "    if len(fp_idxs) > 0:\n",
    "        F_of_fps = batch_rnn_fun(fps)\n",
    "    else:\n",
    "        F_of_fps = onp.zeros([0,n])\n",
    "\n",
    "    all_fps[tol] = {'fps' : fps, 'candidates' : fp_candidates[fp_idxs],\n",
    "                    'losses' : fp_losses, 'F_of_fps' : F_of_fps, \n",
    "                    'opt_details' : fp_opt_details, 'hps' : fp_hps}\n",
    "\n",
    "    all_fps[tol]\n",
    "#except:\n",
    "#    b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the quality of the fixed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure(figsize=(12,6))\n",
    "\n",
    "for tol in fp_tols: \n",
    "    plt.semilogy(all_fps[tol]['losses']); \n",
    "    plt.xlabel('Fixed point #')\n",
    "    plt.ylabel('Fixed point loss');\n",
    "plt.legend(fp_tols)\n",
    "plt.title('Fixed point loss by fixed point (sorted) and stop tolerance')\n",
    "\n",
    "f2 = plt.figure(figsize=(12,4))\n",
    "\n",
    "pidx = 1\n",
    "nfp_tols = len(fp_tols)\n",
    "for tol_idx, tol in enumerate(fp_tols):\n",
    "    plt.subplot(1, nfp_tols, pidx); pidx += 1\n",
    "    plt.hist(onp.log10(fp_loss_fun(all_fps[tol]['fps'])), 50);\n",
    "    plt.xlabel('log10(FP loss)')\n",
    "    plt.title('Tolerance: ' + str(tol));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get a nice representation of the line using the fixed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the best fixed points by projection onto the readoud.\n",
    "best_tol = fp_tols[-1]\n",
    "fps = all_fps[best_tol]['fps']\n",
    "fp_readouts = onp.squeeze(onp.dot(params['wO'], fps.T) + onp.expand_dims(params['bO'], axis=1))\n",
    "fp_ro_sidxs = onp.argsort(fp_readouts)\n",
    "sorted_fp_readouts = fp_readouts[fp_ro_sidxs]\n",
    "sorted_fps = fps[fp_ro_sidxs]\n",
    "\n",
    "downsample_fps = 2 # Use this if too many fps\n",
    "sorted_fp_readouts = sorted_fp_readouts[0:-1:downsample_fps]\n",
    "print(len(sorted_fp_readouts))\n",
    "sorted_fps = sorted_fps[0:-1:downsample_fps]\n",
    "jacs = fp_optimize.compute_jacobians(rnn_fun, sorted_fps)\n",
    "eig_decomps = fp_optimize.compute_eigenvalue_decomposition(jacs, sort_by='real', do_compute_lefts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the system starting at these fixed points, without input, and make sure the system is at equilibrium there. Note one can have fixed points that are very unstable, but that does not show up in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_inputs_no_targets_h0s(keys):\n",
    "    nkeys = len(keys)\n",
    "    inputs_b = np.zeros([nkeys, ntimesteps, u])\n",
    "    targets_b = np.zeros([nkeys, ntimesteps, o]) \n",
    "    h0s_b = sorted_fps[:nkeys,:]\n",
    "    masks_b = None\n",
    "    return inputs_b, targets_b, masks_b, h0s_b\n",
    "\n",
    "rnn_run = lambda inputs_b, h0s_b: rnn.batched_rnn_run_w_h0(params, inputs_b, h0s_b)\n",
    "\n",
    "nexamples = len(sorted_fps)\n",
    "rnn_internals_slow = rnn.run_trials(rnn_run, no_inputs_no_targets_h0s, 1, nexamples)\n",
    "\n",
    "rnn.plot_examples(ntimesteps, rnn_internals_slow, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now, through a series of plots and dot products, we will see how the GRU solved the binary decision task. First we plot the fixed points, the fixed point candidates that the fixed point optimization was seeded with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Black shows the original candidate point, the colored stars show the fixed point, where the color of the fixed point is the projection onto the readout vector and the size is commensurate with how slow it is (slower is larger).\n",
    "\n",
    "* So in this example, we see that the fixed point structure implements an approximate line attractor, which is the one-dimensional manifold likely used to integrate the white noise and ultimately lead to the decision.\n",
    "\n",
    "* Note also the shape of the manifold relative to the color.  The color is the based on the readout value of the fixed point, so it appears that there may be three parts to the line attractor.  The middle and two sides.  The two sides may be integrating, even though the the readout would be +1 or -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "fig = plt.figure(figsize=(16,16));\n",
    "ax = fig.add_subplot(111, projection='3d');\n",
    "\n",
    "pca = PCA(n_components=3).fit(fp_candidates)\n",
    "\n",
    "\n",
    "max_fps_to_plot = 1000\n",
    "sizes = [100, 500]\n",
    "for tol, size in zip(fp_tols[1:2], sizes):\n",
    "    hiddens = all_fps[tol]['candidates']\n",
    "\n",
    "    h_pca = pca.transform(hiddens)\n",
    "\n",
    "    emax = h_pca.shape[0] if h_pca.shape[0] < max_fps_to_plot else max_fps_to_plot\n",
    "\n",
    "    alpha = 0.01\n",
    "    ax.scatter(h_pca[0:emax,0], h_pca[0:emax,1], h_pca[0:emax,2], color=[0, 0, 0, 0.1], s=10)\n",
    "\n",
    "    hstars = np.reshape(all_fps[tol]['fps'], (-1, n))\n",
    "    hstar_pca = pca.transform(hstars)\n",
    "    color = onp.squeeze(onp.dot(params['wO'], hstars.T) + onp.expand_dims(params['bO'], axis=1))\n",
    "    color = onp.where(color > 1.0, 1.0, color)\n",
    "    color = onp.where(color < -1.0, -1.0, color)\n",
    "    color = (color + 1.0) / 2.0    \n",
    "    \n",
    "    marker_style = dict(marker='*', s=size, edgecolor='gray')\n",
    "    \n",
    "    ax.scatter(hstar_pca[0:emax,0], hstar_pca[0:emax,1], hstar_pca[0:emax,2], \n",
    "                c=color[0:emax], **marker_style);\n",
    "\n",
    "    for eidx in range(emax):\n",
    "        ax.plot3D([h_pca[eidx,0], hstar_pca[eidx,0]], \n",
    "                  [h_pca[eidx,1], hstar_pca[eidx,1]],\n",
    "                  [h_pca[eidx,2], hstar_pca[eidx,2]], c=[0, 0, 1, alpha])    \n",
    "        \n",
    "plt.title('Fixed point structure and fixed point candidate starting points.');\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_zlabel('PC 3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a look at the fixed points, and the trajectories started at the fixed points, without any input, all plotted in the 3D PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16));\n",
    "ax = fig.add_subplot(111, projection='3d');\n",
    "\n",
    "\n",
    "all_hiddens = onp.reshape(rnn_internals_slow['hiddens'], (-1, n))\n",
    "pca = PCA(n_components=3).fit(fp_candidates)\n",
    "\n",
    "alpha = 0.05\n",
    "emax = nexamples\n",
    "for eidx in range(emax):\n",
    "    h_pca = pca.transform(rnn_internals_slow['hiddens'][eidx,:,:])\n",
    "    ax.plot3D(h_pca[:,0], \n",
    "              h_pca[:,1],\n",
    "              h_pca[:,2], c=[0, 0, 1, alpha])    \n",
    "\n",
    "        \n",
    "size = 100\n",
    "hstar_pca = pca.transform(sorted_fps)\n",
    "color = onp.squeeze(onp.dot(params['wO'], sorted_fps.T) + onp.expand_dims(params['bO'], axis=1))\n",
    "color = onp.where(color > 1.0, 1.0, color)\n",
    "color = onp.where(color < -1.0, -1.0, color)\n",
    "color = (color + 1.0) / 2.0    \n",
    "marker_style = dict(marker='*', s=size, edgecolor='gray')\n",
    "\n",
    "\n",
    "ax.scatter(hstar_pca[0:emax,0], hstar_pca[0:emax,1], hstar_pca[0:emax,2], \n",
    "           c=color[0:emax], **marker_style);\n",
    "\n",
    "plt.title('High quality fixed points and the network dynamics initialized from them.');\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_zlabel('PC 3');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of linearized systems around the fixed points.\n",
    "\n",
    "Glancing up at the trained parameters plot, you can see the eigenvalues of the GRU linearized around the trained initial condition, $h_0$. These eigenvalues are plotted in the complex plane.  There is one eigenvalue very close to $(1,0)$ in the complex plane, this means the system can integrate.  The rest of the eigenvalues are within the unit circle, meaning they are stable, decaying modes.  For this example, we can safely ignore all the modes except the first one.\n",
    "\n",
    "Below, we plot the top eigenvalues as a function of the location on the readout. The top eigenvalue is very close to $(1,0)$ across the line readout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigs = 3\n",
    "plt.figure(figsize=(neigs*5, 3))\n",
    "for eidx in range(neigs):\n",
    "    max_eigs = []\n",
    "    for decomp in eig_decomps:\n",
    "        evals = decomp['evals']\n",
    "        max_eigs.append(onp.real(evals[eidx]))\n",
    "\n",
    "    max_eigs = onp.array(max_eigs)\n",
    "\n",
    "    plt.subplot(1,neigs,eidx+1)\n",
    "    plt.scatter(sorted_fp_readouts, max_eigs, c=sorted_fp_readouts);\n",
    "    plt.plot([-1,1,], [1, 1], 'k')\n",
    "    plt.axis('tight')\n",
    "    plt.title('Eigenvalue # ' + str(eidx))\n",
    "    plt.xlabel('Readout projection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another major quantity of interest is what the right and left eigenvectors are doing.\n",
    "\n",
    "Here, we will comment exclusively on the right eigenvectors.  The right eigenvectors give the direction in which the system will integrate input.  Projecting the right eigenvectors on the readout of the GRU is a very natural thing to do then, because it shows when input is integrated to move the readout (if the readout of the right eigenvector and the readout is high), vs. when the input is integrated and does not change the readout projection (if the readout of the right eigenvector and the readout is very small, basically orthogonal).\n",
    "\n",
    "Notice that the projection between the right maximal eigenvalue and the readout varies as a function of the location of the fixed point.  This is very curious and points to how the nonlinear GRU is solving the binary decision task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldots = []\n",
    "rdots = []\n",
    "rdotla = []\n",
    "\n",
    "color = onp.squeeze(onp.dot(params['wO'], sorted_fps.T) + onp.expand_dims(params['bO'], axis=1))\n",
    "color = onp.where(color > 1.0, 1.0, color)\n",
    "color = onp.where(color < -1.0, -1.0, color)\n",
    "color = (color + 1.0) / 2.0    \n",
    "\n",
    "nfps = len(sorted_fps)\n",
    "for jidx in range(nfps):\n",
    "    fp = sorted_fps[jidx, :]\n",
    "    rnn_fun_x = lambda x : rnn.gru(params, fp, x)\n",
    "    dfdx = jacrev(rnn_fun_x)\n",
    "    r0 = onp.real(eig_decomps[jidx]['R'][:, 0])                          \n",
    "    rdots.append(onp.dot(r0, params['wO'].T))\n",
    "    l0 = onp.real(eig_decomps[jidx]['L'][:, 0])\n",
    "    ldots.append(onp.dot(l0, dfdx(onp.ones([1]))))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(sorted_fp_readouts, onp.abs(rdots), c=color)\n",
    "plt.title('Rights dotted with readout')\n",
    "plt.subplot(122)\n",
    "plt.scatter(sorted_fp_readouts, onp.abs(ldots), c=color)\n",
    "plt.title('Lefts dotted with effective input')\n",
    "plt.ylim([0, 3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three other sets of dot products give a nearly complete story. \n",
    "\n",
    "1. Dot product of fixed points with the readout as a function of where the fixed point is on the line attractor.  They are either very high, or very low.  \n",
    "2. Dot product of the local direction of the line attractor (the tangent of the line) and readout.  This shows that most of the line attractor motion is orthogonal to the readout, thus implementing something approximating a decision boundary. \n",
    "3. Dot product of the right maximal eigenvector with the local direction of the line attractor. While a bit messy, this shows that the direction local integration, as given by the right maximal eigenvector, is always lined up with the line attractor tangent, _regardless_ of the projection onto the readout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdotsla = []\n",
    "la_dots = []\n",
    "la_locs = []\n",
    "la_path_int = [0.0]\n",
    "naround = 3\n",
    "la_last = sorted_fps[naround-1,:]\n",
    "for jidx in range(naround, nfps-naround):\n",
    "    idxs = onp.arange(jidx-naround, jidx+naround+1)\n",
    "    v1 = sorted_fps[idxs[0],:]\n",
    "    v2 = sorted_fps[idxs[-1],:]\n",
    "    la = (v2-v1)/onp.linalg.norm(v2-v1) # approximate line attractor direction.\n",
    "    la_dots.append(onp.dot(la, params['wO'].T))\n",
    "    la_locs.append(onp.squeeze(onp.dot(onp.mean(sorted_fps[idxs,:], axis=0), params['wO'].T) + params['bO']))\n",
    "    la_path_int.append(la_path_int[-1] + onp.linalg.norm(la-la_last))\n",
    "    la_last = la\n",
    "\n",
    "    r0 = onp.real(eig_decomps[jidx]['R'][:, 0])\n",
    "    rdotsla.append(onp.abs(onp.dot(r0, la.T)))\n",
    "\n",
    "    \n",
    "la_dots = onp.array(la_dots)\n",
    "la_locs = onp.array(la_locs)\n",
    "la_path_int = onp.array(la_path_int)\n",
    "la_path_int = la_path_int[1:]\n",
    "\n",
    "color2 = color[naround: -naround]\n",
    "    \n",
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(131)\n",
    "plt.scatter(la_locs, la_dots, c=color2)\n",
    "plt.xlabel('Readout of fixed point location')\n",
    "plt.title('Line attractor direction dotted with readout')\n",
    "plt.subplot(132)\n",
    "plt.scatter(la_path_int, la_dots, c=color2)\n",
    "plt.xlabel('Line attractor path integral')\n",
    "plt.title('Line attractor direction dotted with readout');\n",
    "plt.subplot(133)\n",
    "plt.scatter(la_path_int, rdotsla)\n",
    "plt.xlabel('Line attractor path integral')\n",
    "plt.title('Line attractor direction dotted with right 0 eigenvector');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the GRU decision task.\n",
    "So it seems pretty clear how the system solved this decision task. \n",
    "* The GRU created a 1-dimensional manifold of fixed points, also known as a line attractor.  This line attractor is good at holding an analog memory, such as the integral of white noise input. \n",
    "* Local linear dynamics integrated the input along the line attractor.\n",
    "* The GRU then __bent__ and __aligned__ that line attractor, such that most path was orthogonal to the readout vector.  \n",
    "* Thus, the GRU could jump from +1 to -1 readout, while still adjusting it's confidence of the decision, based on the white noise integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment\n",
    "\n",
    "Change the error of the readout to cross-entropy error, drop the mean-squared error.  What changes, what stays the same?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
